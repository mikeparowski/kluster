{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_corpus\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class Vectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vectorizer='count', ngram_range=(1,1), max_df=1.0, min_df=1, norm='l2'):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.norm = norm\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        if self.vectorizer == 'tfidf':\n",
    "            vec = TfidfVectorizer(ngram_range=self.ngram_range, max_df=self.max_df, min_df=self.min_df, norm=self.norm)\n",
    "        else:\n",
    "            vec = CountVectorizer(ngram_range=self.ngram_range, max_df=self.max_df, min_df=self.min_df)\n",
    "        return vec.fit_transform(X)\n",
    "\n",
    "class OptionalSVD(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, compute=False, n_components=2):\n",
    "        self.compute = compute\n",
    "        self.n_components = n_components\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        if self.compute:\n",
    "            svd = TruncatedSVD(n_components=self.n_components, random_state=42).fit(X)\n",
    "            self.components_ = svd.components_\n",
    "            self.explained_variance_ = svd.explained_variance_\n",
    "            self.explained_variance_ratio_ = svd.explained_variance_ratio_\n",
    "            self.singular_values_ = svd.singular_values_\n",
    "            X = svd.transform(X)\n",
    "        return X\n",
    "    \n",
    "class OptionalScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, scale=False):\n",
    "        self.scale = scale\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        if self.scale:\n",
    "            X = StandardScaler().fit_transform(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sources = (('../data/articles/breitbart', 'bb'), ('../data/articles/thinkprogress', 'tp'))\n",
    "data = build_corpus(sources)\n",
    "X = data['text'].copy()\n",
    "y = data['class'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = 'count'\n",
    "ngram_range = (1,1)\n",
    "max_df = 1.0\n",
    "min_df = 1\n",
    "norm = 'l2'\n",
    "reduce = True\n",
    "n_components = 100\n",
    "scale = False\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', Vectorizer(vectorizer, ngram_range, max_df, min_df, norm)),\n",
    "    ('reducer', OptionalSVD(reduce, n_components)),\n",
    "    ('scaler', OptionalScaler(scale)),\n",
    "    ('estimator', LinearRegression())\n",
    "])\n",
    "\n",
    "#data_prepared = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4382944  0.40974743 0.38050611 0.36649439 0.41808696 0.40623127\n",
      " 0.38003281 0.38540035 0.38216054 0.38572819]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "scores = cross_val_score(lin_reg, data_prepared, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "lin_rmse_scores = np.sqrt(-scores)\n",
    "print(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.4382944  0.40974743 0.38050611 0.36649439 0.41808696 0.40623127\n",
      " 0.38003281 0.38540035 0.38216054 0.38572819]\n",
      "Mean: 0.3952682436131147\n",
      "Standard deviation: 0.020832552827688793\n"
     ]
    }
   ],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#param_grid = [\n",
    "#    {'vectorizer':['count', 'tfidf'], 'ngram_range':[(1,1), (1,2), (1,3)], 'max_df': [1.0, 0.9,0.8], 'min_df': [1, 0.05, 0.2], \n",
    "#     'reduce': [True,False], 'n_components': [100,200,300], 'scale': [True, False]}\n",
    "#]\n",
    "\n",
    "param_grid = [\n",
    "    {'vect__vectorizer': ['tfidf'], 'vect__max_df': (1.0, 0.9, 0.8), 'vect__min_df': (1, 0.05, 0.2)},\n",
    "    {'vect__vectorizer': ['tfidf'], 'reducer__reduce': (True,False), 'scaler__scale': (True,False), 'scaler__n_components': (100,200,300)}\n",
    "]\n",
    "#param_grid = {\n",
    "#    'vect__vectorizer': ('count', 'tfidf')\n",
    "#}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', Vectorizer(max_df=1.0, min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "      vectorizer='count')), ('reducer', OptionalSVD(compute=True, n_components=100)), ('scaler', OptionalScaler(scale=False)), ('estimator', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
